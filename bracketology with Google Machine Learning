-- 4th lab of 5 in "create ML models with bigquery" course

-- task 1 opeing big query console - very basic
-- task 2 examine NCSS March madness - data about college basketball tournaments
-- task 3 adding public datasets - either by searching in "Public Datasets" or adding project "bigquery-public-data" as a "starred" project
-- starring a project appears to mean having it listed, along with its datasets and tables therein, in BigQuery Studio/console
-- task 4 - write query to detmine available seasons and games

SELECT
  season,
  COUNT(*) as games_per_tournament
  FROM
 `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`
 GROUP BY season
 ORDER BY season # default is Ascending (low to high)

 -- task 5 - understanding machine learning features and labels
-- inlcudes quesitions about what we know to build the model and what we need to predict

-- task 6 - create a labelled machine learning dataset
-- the table has one record per game and tells you who won and who lost in it
-- for our ml project, we need 2 records per game - one for the winner and another for the loser
-- this query creates a new table structure that can be used. Note the UNION ALL to get winner record then loser record...
-- and also us creating the "win or lose" labelled column (binary classifier) to use to train on
# create a row for the winning team
SELECT
  # features
  season, # ex: 2015 season has March 2016 tournament games
  round, # sweet 16
  days_from_epoch, # how old is the game
  game_date,
  day, # Friday

  'win' AS label, # our label

  win_seed AS seed, # ranking
  win_market AS market,
  win_name AS name,
  win_alias AS alias,
  win_school_ncaa AS school_ncaa,
  # win_pts AS points,

  lose_seed AS opponent_seed, # ranking
  lose_market AS opponent_market,
  lose_name AS opponent_name,
  lose_alias AS opponent_alias,
  lose_school_ncaa AS opponent_school_ncaa
  # lose_pts AS opponent_points

FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`

UNION ALL

# create a separate row for the losing team
SELECT
# features
  season,
  round,
  days_from_epoch,
  game_date,
  day,

  'loss' AS label, # our label

  lose_seed AS seed, # ranking
  lose_market AS market,
  lose_name AS name,
  lose_alias AS alias,
  lose_school_ncaa AS school_ncaa,
  # lose_pts AS points,

  win_seed AS opponent_seed, # ranking
  win_market AS opponent_market,
  win_name AS opponent_name,
  win_alias AS opponent_alias,
  win_school_ncaa AS opponent_school_ncaa
  # win_pts AS opponent_points

FROM
`bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`

-- create the machine leanring model (iteration 1) with BigQuery ML

CREATE OR REPLACE MODEL
  `bracketology.ncaa_model`
OPTIONS
  ( model_type='logistic_reg') AS

# create a row for the winning team
SELECT
  # features
  season,

  'win' AS label, # our label

  win_seed AS seed, # ranking
  win_school_ncaa AS school_ncaa,

  lose_seed AS opponent_seed, # ranking
  lose_school_ncaa AS opponent_school_ncaa

FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`
WHERE season <= 2017

UNION ALL

# create a separate row for the losing team
SELECT
# features
  season,

  'loss' AS label, # our label
  lose_seed AS seed, # ranking
  lose_school_ncaa AS school_ncaa,

  win_seed AS opponent_seed, # ranking
  win_school_ncaa AS opponent_school_ncaa

FROM
`bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`

/* now we split our dataset with a WHERE clause so we can train on a subset of data and then evaluate and test the model's performance against a reserved subset so the model doesn't memorize or overfit to the training data.

# tournament season information from 1985 - 2017
# here we'll train on 1985 - 2017 and predict for 2018
*/
WHERE season <= 2017

-- after viewing the model training stats we can see what the model learned about the features we used by viuewing the weights (to see if they are significant weights)

SELECT
  category,
  weight
FROM
  UNNEST((
    SELECT
      category_weights
    FROM
      ML.WEIGHTS(MODEL `bracketology.ncaa_model`)
    WHERE
      processed_input = 'seed')) # try other features like 'school_ncaa'
      ORDER BY weight DESC

/* result set
category	weight
01	0.58388374410427812
02	0.38294739129505906
03	0.25638072363517889
04	0.083681848292407643
06	0.045950116454151271
05	-0.0070051544303288624
07	-0.015531722291875997
08	-0.15311398815069865
11	-0.1756164632813598
10	-0.2104315941591508
12	-0.25375053625834781
09	-0.27464763340381
13	-0.54998513880110145
16	-0.65618301509117227
14	-0.66040504525895982
15	-0.71854237472467974
*/
-- not sure how we know which 'category' is whihc 'feature'...?

-- task 8 - evaluate model performance--
SELECT
  *
FROM
  ML.EVALUATE(MODEL   `bracketology.ncaa_model`)

  /*
precision	recall	accuracy	f1_score	log_loss	roc_auc
0.712468193384224	0.66193853427895977	0.69341317365269461	0.68627450980392157	0.57842239934512185	0.76591108891108894
  */

-- precision = 71%
-- recall = 66%
-- accuracy = 69%
-- f1 score = 69%
-- log loss = 58%
-- reciever operation characteristic - area under curve (roc_auc) = 77%

-- task 9 - making predictions (also know as inferencing)

-- here we use the ML.PREDICT() sql function - the output is a dataset you can insert directly into a table (using a "create table as select...." style query)

CREATE OR REPLACE TABLE `bracketology.predictions` AS (

SELECT * FROM ML.PREDICT(MODEL `bracketology.ncaa_model`,

# predicting for 2018 tournament games (2017 season)
(SELECT * FROM `data-to-insights.ncaa.2018_tournament_results`)
)
)

-- task 10 - how many did our model get right for the 2018 NCAA tournament?

SELECT * FROM `bracketology.predictions`
WHERE predicted_label <> label

-- task 11 - models can only take you so far...
-- this query picks out and repsents games that the model predicted incorrectly and was more than 80% confident it had predicted correctly

SELECT
  model.label AS predicted_label,
  model.prob AS confidence,

  predictions.label AS correct_label,

  game_date,
  round,

  seed,
  school_ncaa,
  points,

  opponent_seed,
  opponent_school_ncaa,
  opponent_points

FROM `bracketology.predictions` AS predictions,
UNNEST(predicted_label_probs) AS model

WHERE model.prob > .8 AND predicted_label <> predictions.label

-- task 12 - using skillful ML model features

-- create a new ML dataset with new, additional feaures

# create training dataset:
# create a row for the winning team
CREATE OR REPLACE TABLE `bracketology.training_new_features` AS
WITH outcomes AS (
SELECT
  # features
  season, # 1994

  'win' AS label, # our label

  win_seed AS seed, # ranking # this time without seed even
  win_school_ncaa AS school_ncaa,

  lose_seed AS opponent_seed, # ranking
  lose_school_ncaa AS opponent_school_ncaa

FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games` t
WHERE season >= 2014

UNION ALL

# create a separate row for the losing team
SELECT
# features
  season, # 1994

  'loss' AS label, # our label

  lose_seed AS seed, # ranking
  lose_school_ncaa AS school_ncaa,

  win_seed AS opponent_seed, # ranking
  win_school_ncaa AS opponent_school_ncaa

FROM
`bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games` t
WHERE season >= 2014

UNION ALL

# add in 2018 tournament game results not part of the public dataset:
SELECT
  season,
  label,
  seed,
  school_ncaa,
  opponent_seed,
  opponent_school_ncaa
FROM
  `data-to-insights.ncaa.2018_tournament_results`

)

SELECT
o.season,
label,

# our team
  seed,
  school_ncaa,
  # new pace metrics (basketball possession)
  team.pace_rank,
  team.poss_40min,
  team.pace_rating,
  # new efficiency metrics (scoring over time)
  team.efficiency_rank,
  team.pts_100poss,
  team.efficiency_rating,

# opposing team
  opponent_seed,
  opponent_school_ncaa,
  # new pace metrics (basketball possession)
  opp.pace_rank AS opp_pace_rank,
  opp.poss_40min AS opp_poss_40min,
  opp.pace_rating AS opp_pace_rating,
  # new efficiency metrics (scoring over time)
  opp.efficiency_rank AS opp_efficiency_rank,
  opp.pts_100poss AS opp_pts_100poss,
  opp.efficiency_rating AS opp_efficiency_rating,

# a little feature engineering (take the difference in stats)

  # new pace metrics (basketball possession)
  opp.pace_rank - team.pace_rank AS pace_rank_diff,
  opp.poss_40min - team.poss_40min AS pace_stat_diff,
  opp.pace_rating - team.pace_rating AS pace_rating_diff,
  # new efficiency metrics (scoring over time)
  opp.efficiency_rank - team.efficiency_rank AS eff_rank_diff,
  opp.pts_100poss - team.pts_100poss AS eff_stat_diff,
  opp.efficiency_rating - team.efficiency_rating AS eff_rating_diff

FROM outcomes AS o
LEFT JOIN `data-to-insights.ncaa.feature_engineering` AS team
ON o.school_ncaa = team.team AND o.season = team.season
LEFT JOIN `data-to-insights.ncaa.feature_engineering` AS opp
ON o.opponent_school_ncaa = opp.team AND o.season = opp.season

-- task 15 - train teh new model

CREATE OR REPLACE MODEL
  `bracketology.ncaa_model_updated`
OPTIONS
  ( model_type='logistic_reg') AS

SELECT
  # this time, dont train the model on school name or seed
  season,
  label,

  # our pace
  poss_40min,
  pace_rank,
  pace_rating,

  # opponent pace
  opp_poss_40min,
  opp_pace_rank,
  opp_pace_rating,

  # difference in pace
  pace_rank_diff,
  pace_stat_diff,
  pace_rating_diff,


  # our efficiency
  pts_100poss,
  efficiency_rank,
  efficiency_rating,

  # opponent efficiency
  opp_pts_100poss,
  opp_efficiency_rank,
  opp_efficiency_rating,

  # difference in efficiency
  eff_rank_diff,
  eff_stat_diff,
  eff_rating_diff

FROM `bracketology.training_new_features`

# here well train on 2014 - 2017 and predict on 2018
WHERE season BETWEEN 2014 AND 2017 # between in SQL is inclusive of end points

-- task 16 - evaluate the new models performance

SELECT
  *
FROM
  ML.EVALUATE(MODEL     `bracketology.ncaa_model_updated`)

/*
precision	recall	accuracy	f1_score	log_loss	roc_auc
0.75471698113207553	0.8	0.76530612244897955	0.77669902912621358	0.55295203154945083	0.79540659340659337
*/

-- inspect what the model learned
SELECT
  *
FROM
  ML.WEIGHTS(MODEL     `bracketology.ncaa_model_updated`)
ORDER BY ABS(weight) DESC

--result
/*
processed_input	weight	category_weights
__INTERCEPT__	-10.973140286774118	"{
  ""category_weights"": []
}"
pace_stat_diff	0.014969759913783728	"{
  ""category_weights"": []
}"
pts_100poss	0.013785724051538101	"{
  ""category_weights"": []
}"
opp_pts_100poss	-0.013092088355899712	"{
  ""category_weights"": []
}"
eff_stat_diff	-0.012076124900066066	"{
  ""category_weights"": []
}"
opp_poss_40min	0.010549333699694527	"{
  ""category_weights"": []
}"
poss_40min	-0.0088289412487943635	"{
  ""category_weights"": []
}"
efficiency_rating	0.0076381923842803907	"{
  ""category_weights"": []
}"
eff_rating_diff	-0.0064428704146661225	"{
  ""category_weights"": []
}"
opp_efficiency_rating	-0.0061229393055222663	"{
  ""category_weights"": []
}"
season	0.0053473060908411779	"{
  ""category_weights"": []
}"
efficiency_rank	-0.0021759381708546339	"{
  ""category_weights"": []
}"
eff_rank_diff	0.0018370944511204486	"{
  ""category_weights"": []
}"
opp_efficiency_rank	0.0017624544958374346	"{
  ""category_weights"": []
}"
pace_rating	-0.0016677796642053143	"{
  ""category_weights"": []
}"
pace_rating_diff	0.001519756700603435	"{
  ""category_weights"": []
}"
opp_pace_rating	0.0012595851680260604	"{
  ""category_weights"": []
}"
pace_rank	0.00045057931050470235	"{
  ""category_weights"": []
}"
pace_rank_diff	-0.00041853158247261575	"{
  ""category_weights"": []
}"
opp_pace_rank	-0.0003531353775123865	"{
  ""category_weights"": []
}"
*/

-- task 18 prediction time with new model

CREATE OR REPLACE TABLE `bracketology.ncaa_2018_predictions` AS

# lets add back our other data columns for context
SELECT
  *
FROM
  ML.PREDICT(MODEL     `bracketology.ncaa_model_updated`, (

SELECT
* # include all columns now (the model has already been trained)
FROM `bracketology.training_new_features`

WHERE season = 2018

))

-- task 19 - prediction analysis
SELECT * FROM `bracketology.ncaa_2018_predictions`
WHERE predicted_label <> label

-- notice model for 48 records wrong (24 games)

-- task 20 - where were the upsets in 2018?

SELECT
CONCAT(school_ncaa, " was predicted to ",IF(predicted_label="loss","lose","win")," ",CAST(ROUND(p.prob,2)*100 AS STRING), "% but ", IF(n.label="loss","lost","won")) AS narrative,
predicted_label, # what the model thought
n.label, # what actually happened
ROUND(p.prob,2) AS probability,
season,

# us
seed,
school_ncaa,
pace_rank,
efficiency_rank,

# them
opponent_seed,
opponent_school_ncaa,
opp_pace_rank,
opp_efficiency_rank

FROM `bracketology.ncaa_2018_predictions` AS n,
UNNEST(predicted_label_probs) AS p
WHERE
  predicted_label <> n.label # model got it wrong
  AND p.prob > .75  # by more than 75% confidence
ORDER BY prob DESC

-- task 21 - comparing model performance

SELECT
CONCAT(opponent_school_ncaa, " (", opponent_seed, ") was ",CAST(ROUND(ROUND(p.prob,2)*100,2) AS STRING),"% predicted to upset ", school_ncaa, " (", seed, ") and did!") AS narrative,
predicted_label, # what the model thought
n.label, # what actually happened
ROUND(p.prob,2) AS probability,
season,

# us
seed,
school_ncaa,
pace_rank,
efficiency_rank,

# them
opponent_seed,
opponent_school_ncaa,
opp_pace_rank,
opp_efficiency_rank,

(CAST(opponent_seed AS INT64) - CAST(seed AS INT64)) AS seed_diff

FROM `bracketology.ncaa_2018_predictions` AS n,
UNNEST(predicted_label_probs) AS p
WHERE
  predicted_label = 'loss'
  AND predicted_label = n.label # model got it right
  AND p.prob >= .55  # by 55%+ confidence
  AND (CAST(opponent_seed AS INT64) - CAST(seed AS INT64)) > 2 # seed difference magnitude
ORDER BY (CAST(opponent_seed AS INT64) - CAST(seed AS INT64)) DESC

-- task 22 - predictng for the 2019 March Madness tournament
-- first explore the data:
SELECT * FROM `data-to-insights.ncaa.2019_tournament_seeds` WHERE seed = 1

-- practice creating a matrix of all possible games:
SELECT
  NULL AS label,
  team.school_ncaa AS team_school_ncaa,
  team.seed AS team_seed,
  opp.school_ncaa AS opp_school_ncaa,
  opp.seed AS opp_seed
FROM `data-to-insights.ncaa.2019_tournament_seeds` AS team
CROSS JOIN `data-to-insights.ncaa.2019_tournament_seeds` AS opp
# teams cannot play against themselves :)
WHERE team.school_ncaa <> opp.school_ncaa

-- then create the table adding in a couple of other columns - namely pace and efficiency scores for the previous seson (2018)

CREATE OR REPLACE TABLE `bracketology.ncaa_2019_tournament` AS

WITH team_seeds_all_possible_games AS (
  SELECT
    NULL AS label,
    team.school_ncaa AS school_ncaa,
    team.seed AS seed,
    opp.school_ncaa AS opponent_school_ncaa,
    opp.seed AS opponent_seed
  FROM `data-to-insights.ncaa.2019_tournament_seeds` AS team
  CROSS JOIN `data-to-insights.ncaa.2019_tournament_seeds` AS opp
  # teams cannot play against themselves :)
  WHERE team.school_ncaa <> opp.school_ncaa
)

, add_in_2018_season_stats AS (
SELECT
  team_seeds_all_possible_games.*,
  # bring in features from the 2018 regular season for each team
  (SELECT AS STRUCT * FROM `data-to-insights.ncaa.feature_engineering` WHERE school_ncaa = team AND season = 2018) AS team,
  (SELECT AS STRUCT * FROM `data-to-insights.ncaa.feature_engineering` WHERE opponent_school_ncaa = team AND season = 2018) AS opp

FROM team_seeds_all_possible_games
)

# Preparing 2019 data for prediction
SELECT

  label,

  2019 AS season, # 2018-2019 tournament season

# our team
  seed,
  school_ncaa,
  # new pace metrics (basketball possession)
  team.pace_rank,
  team.poss_40min,
  team.pace_rating,
  # new efficiency metrics (scoring over time)
  team.efficiency_rank,
  team.pts_100poss,
  team.efficiency_rating,

# opposing team
  opponent_seed,
  opponent_school_ncaa,
  # new pace metrics (basketball possession)
  opp.pace_rank AS opp_pace_rank,
  opp.poss_40min AS opp_poss_40min,
  opp.pace_rating AS opp_pace_rating,
  # new efficiency metrics (scoring over time)
  opp.efficiency_rank AS opp_efficiency_rank,
  opp.pts_100poss AS opp_pts_100poss,
  opp.efficiency_rating AS opp_efficiency_rating,

# a little feature engineering (take the difference in stats)

  # new pace metrics (basketball possession)
  opp.pace_rank - team.pace_rank AS pace_rank_diff,
  opp.poss_40min - team.poss_40min AS pace_stat_diff,
  opp.pace_rating - team.pace_rating AS pace_rating_diff,
  # new efficiency metrics (scoring over time)
  opp.efficiency_rank - team.efficiency_rank AS eff_rank_diff,
  opp.pts_100poss - team.pts_100poss AS eff_stat_diff,
  opp.efficiency_rating - team.efficiency_rating AS eff_rating_diff

FROM add_in_2018_season_stats

-- and finally make the predictions - first make a table of them

CREATE OR REPLACE TABLE `bracketology.ncaa_2019_tournament_predictions` AS

SELECT
  *
FROM
  # let's predicted using the newer model
  ML.PREDICT(MODEL     `bracketology.ncaa_model_updated`, (

# let's predict on March 2019 tournament games:
SELECT * FROM `bracketology.ncaa_2019_tournament`
))

-- and then get them to lookat them

SELECT
  p.label AS prediction,
  ROUND(p.prob,3) AS confidence,
  school_ncaa,
  seed,
  opponent_school_ncaa,
  opponent_seed
FROM `bracketology.ncaa_2019_tournament_predictions`,
UNNEST(predicted_label_probs) AS p
WHERE p.prob >= .5
AND school_ncaa = 'Duke'
ORDER BY seed, opponent_seed

-- top 5 results look like this - although its a classification model, it shows confidence scores
/*
prediction	confidence	school_ncaa	seed	opponent_school_ncaa	opponent_seed
win	0.515	Duke	1	Gonzaga	1
win	0.605	Duke	1	North Carolina	1
loss	0.694	Duke	1	Virginia	1
loss	0.585	Duke	1	Michigan	2
loss	0.53	Duke	1	Tennessee	2
*/